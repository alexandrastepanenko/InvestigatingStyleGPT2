# GPT-2 Style: How well does generated data replicate the stylistic features of training data?

## Project Summary

This project explores natural language generation via large language models, and how well the stylistic features of text used to fine-tune a model are replicated in generated text. Political news articles are scraped from three UK news websites and compiled into corpora of training data.  The 'small' (124 million parameters) GPT-2 model is fine-tuned on these corpora and used to generate text in the format of news articles. The stylistic features of both real and generated news articles are analysed and compared through visualisation and statistical methods. 

This project was completed as part of an MSc in Data Science at Goldsmiths, University of London.
